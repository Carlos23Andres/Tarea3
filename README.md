# ğŸŒ€ README: OptimizaciÃ³n por Enjambre de PartÃ­culas (PSO)

Basado en Freitas, D., Lopes, L. G., & Morgado-Dias, F. (2020). Particle Swarm Optimisation: A Historical Review up to the Current Developments. Entropy, 22(3), 362.

## ğŸ“– IntroducciÃ³n

El algoritmo de OptimizaciÃ³n por Enjambre de PartÃ­culas (PSO) fue introducido en 1995 por el ingeniero elÃ©ctrico Russell C. Eberhart y el psicÃ³logo social James Kennedy. Su inspiraciÃ³n provino de la observaciÃ³n del comportamiento colectivo en bandadas de aves buscando comida. Este modelo computacional simula la colaboraciÃ³n social, donde un conjunto de partÃ­culas (individuos) trabaja conjuntamente para encontrar soluciones Ã³ptimas en un espacio de bÃºsqueda multidimensional.

A diferencia de otros algoritmos evolutivos como los Algoritmos GenÃ©ticos (GA), el PSO no utiliza operadores de cruce o mutaciÃ³n. En su lugar, se basa en:

- Memoria individual de cada partÃ­cula.

- Intercambio de informaciÃ³n social dentro del enjambre.

- ActualizaciÃ³n de posiciones y velocidades de forma iterativa.

## ğŸ§¬ 1. Origen BiolÃ³gico e InspiraciÃ³n

El PSO surge del estudio de:

- Comportamientos colectivos naturales (aves, peces, insectos).

- Procesos sociales de cooperaciÃ³n.

- TransmisiÃ³n de informaciÃ³n entre individuos.

ğŸ¦ En esencia, cada partÃ­cula â€œaprendeâ€ tanto de su propio Ã©xito como del Ã©xito de las demÃ¡s.

Los investigadores observaron que la cooperaciÃ³n entre individuos simples genera comportamientos colectivos complejos.
De esta observaciÃ³n surgiÃ³ un modelo matemÃ¡tico que simula cÃ³mo un grupo de agentes puede explorar y optimizar un entorno sin un control central.

## âš™ï¸ 2. Funcionamiento BÃ¡sico

Cada partÃ­cula posee:

- Una posiciÃ³n â†’ posible soluciÃ³n.

- Una velocidad â†’ direcciÃ³n y magnitud del cambio.

- Una memoria de su mejor posiciÃ³n.

InformaciÃ³n del mejor individuo global.

## ğŸ§© ActualizaciÃ³n de las partÃ­culas

1. ActualizaciÃ³n de velocidad:

![Interfaz Inicial](https://raw.githubusercontent.com/Carlos23Andres/Tarea3/main/Imagenes/Captura%20desde%202025-10-16%2019-20-33.png)

2. ActualizaciÃ³n de posiciÃ³n:

![Interfaz Inicial](https://raw.githubusercontent.com/Carlos23Andres/Tarea3/main/Imagenes/Captura%20desde%202025-10-16%2019-21-16.png)

ğŸ” Este proceso se repite hasta cumplir un criterio de parada (mÃ¡ximo de iteraciones o error mÃ­nimo).



## ğŸ“ˆ 3. EvoluciÃ³n del PSO y Principales Mejoras

### ğŸª¶ a. Control de Convergencia

- Peso de Inercia (Ï‰) â€“ Introducido por Shi y Eberhart (1998).
Controla la exploraciÃ³n (Ï‰ alto) y explotaciÃ³n (Ï‰ bajo).

- Factor de ConstricciÃ³n (K) â€“ Clerc (1999).
Garantiza convergencia estable al escalar las velocidades.

- PSO Adaptativo (APSO) â€“ Zhan et al. (2009).
Ajusta dinÃ¡micamente los parÃ¡metros segÃºn el estado evolutivo del enjambre.

### ğŸŒ b. Estructuras de Vecindad

- gbest (global) â†’ todas las partÃ­culas comparten informaciÃ³n â†’ convergencia rÃ¡pida.

- lbest (local) â†’ comunicaciÃ³n solo entre vecinos â†’ mÃ¡s diversidad.

- Vecindarios dinÃ¡micos (Suganthan, 1999) â†’ el radio de interacciÃ³n aumenta con el tiempo.

### ğŸ§ª c. Hibridaciones y Algoritmos Combinados

- PSO + GA (Algoritmos GenÃ©ticos) â†’ incorpora selecciÃ³n, cruce y mutaciÃ³n.

- PSO + DE (EvoluciÃ³n Diferencial) â†’ fortalece exploraciÃ³n global.

- PSO + SA (Recocido Simulado) â†’ mejora escape de Ã³ptimos locales.

- PSO + ACO / ABC / Cuco â†’ aprovecha estrategias de bÃºsqueda bioinspiradas.


### âš¡ d. ParalelizaciÃ³n y ComputaciÃ³n Distribuida

- PPSO (Parallel PSO) â†’ usa CPUs o GPUs para evaluar partÃ­culas simultÃ¡neamente.

- Estrategias sÃ­ncronas/asÃ­ncronas â†’ balancean eficiencia y precisiÃ³n.

- Implementaciones MapReduce y Load Balancing â†’ optimizan entornos de clÃºster y nube.

ğŸ’» PSO paralelo es esencial para problemas de alta dimensionalidad y big data.


### ğŸ¯ e. Extensiones Especializadas

- MOPSO (Multi-Objective PSO) â†’ maneja mÃºltiples objetivos y genera frentes de Pareto.

- PSO con Restricciones â†’ usa penalizaciones o mÃ©todos de preservaciÃ³n de factibilidad.

- PSO Multimodal â†’ identifica mÃºltiples Ã³ptimos simultÃ¡neamente.

## ğŸ§  4. Aplicaciones Reales

```text

| Campo                         | AplicaciÃ³n                                                |
| ----------------------------- | --------------------------------------------------------- |
| ğŸ§® Redes Neuronales           | Entrenamiento de pesos y estructura.                     |
| ğŸ§¬ BioinformÃ¡tica             | ClasificaciÃ³n y alineamiento genÃ³mico.                       |
| âš¡ IngenierÃ­a                 | DiseÃ±o de antenas, controladores, estructuras.                    |
| ğŸ›°ï¸ RobÃ³tica y Transporte      | PlanificaciÃ³n de rutas, control autÃ³nomo.                       |
| ğŸ’» Procesamiento de SeÃ±ales   | Filtrado adaptativo, anÃ¡lisis de ECG, imÃ¡genes.                  |
| ğŸ­ PlanificaciÃ³n y Scheduling | AsignaciÃ³n de tareas y recursos en sistemas distribuidos.       |

```

## ğŸš€ 5. Pasos para Implementar PSO (GuÃ­a PrÃ¡ctica)

1. Definir la funciÃ³n objetivo â†’ quÃ© se quiere minimizar o maximizar.

2. Inicializar partÃ­culas con posiciones y velocidades aleatorias.

3. Evaluar el fitness de cada partÃ­cula.

4. Actualizar pbest y gbest segÃºn los mejores resultados.

5. Actualizar velocidades y posiciones con las ecuaciones base.

6. Repetir hasta cumplir el criterio de parada.

7. Registrar y visualizar resultados.

ğŸ§® Tip: Ajusta cuidadosamente los parÃ¡metros (Ï‰, Ï†1, Ï†2) para equilibrar exploraciÃ³n y explotaciÃ³n.


## ğŸ“š 6. Conclusiones y Perspectivas Futuras

El PSO sigue evolucionando hacia nuevas fronteras:

- ğŸ¤ IntegraciÃ³n con Aprendizaje Profundo â€“ optimizaciÃ³n de hiperparÃ¡metros y redes neuronales.

- â˜ï¸ ComputaciÃ³n en la Nube y Escalamiento Exascale â€“ PSO distribuido masivo.

- ğŸ§© Explicabilidad (XAI) â€“ PSO aplicado a modelos interpretables.

- ğŸ§  Autoajuste Inteligente â€“ PSO con aprendizaje autÃ³nomo de parÃ¡metros.

## ğŸ“ Referencia Principal

Freitas, D., Lopes, L. G., & Morgado-Dias, F. (2020). Particle Swarm Optimisation: A Historical Review up to the Current Developments. Entropy, 22(3), 362.
DOI: 10.3390/e22030362

## ğŸ‘¥ Autor

- *Carlos AndrÃ©s SuÃ¡rez Torres* â†’ [Carlos23Andres](https://github.com/Carlos23Andres)  
